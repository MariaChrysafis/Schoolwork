% \documentclass{article}
\documentclass[a4paper]{article}
\usepackage{/Users/mariachrysafis/Documents/Schoolwork/18.650/chrysafis}
\graphicspath{ {/} }
\author{Maria Chrysafis}
\date{\today}
\title{18.650 Homework 1}
\begin{document}
\maketitle
\section{Expectation}
\begin{Exercise}
	Suppose we play a game where we start with $c$ dollars. On each play of the game, you either double or halve your money, with equal probability. What is your expected fortune after $n$ trials?
\end{Exercise}
\begin{Solution}
	Let $X_i$ denote the amount of money you have after playing the game $i$ times. When $i = 0$, by definition, $\Prob[X_0 = c] = 1$, and so, $\E[X_0] = c$. When $i > 0$, 
	\begin{align*}
		\E[X_i] = \E \left[ \frac{1}{2} \cdot \left(2 \cdot X_{i - 1} \right) + \frac{1}{2} \cdot \left( \frac{1}{2} \cdot X_i \right) \right] = \E \left[\frac{5}{4} X_{i - 1} \right] = \frac{5}{4} \E \left[ X_{i - 1} \right].
	\end{align*}
	It immediately follows that $\E[X_i] = \left( \frac{5}{4} \right)^i \cdot c$. Thus, after $n$ trials, your expected fortune is $c \cdot \left( \frac{5}{4}\right)^n$.
\end{Solution}
\begin{Exercise}
	Show that $\Var[X] = 0$ if and only if there is a constant $c$ such that $\Prob[X = c]= 1.$
\end{Exercise}
\begin{Solution}
	We first prove the easier direction, namely that if $\Prob[X = c] = 1$, then $\Var[X] = 0$. In this case, $\E[X^2] = c^2$ and $\E[X]^2 = c^2$ too, so $\Var[X] = \E[X^2] - \E[X]^2 = 0$, as desired. 

	As for the other direction, by Jensen's inequality,
	\begin{align*}
		\mathbb{E}[X^2] \ge \mathbb{E}[X]^2 \Longleftrightarrow \Var[X] \ge 0,
	\end{align*}
	with equality holding iff $X$ is constant, i.e. $\Prob[X = c] = 1$ for some $c$. 
\end{Solution}
\begin{Exercise}
	Let $X_1, \ldots , X_n \sim \Unif[0, 1]$ and let $Y_n = \max \left( X_1, \ldots X_n \right)$. Find $\mathbb{E}[Y_n]$.
\end{Exercise}
\begin{Solution}
	Consider the CDF of $Y_n$:
	\begin{align*}
		F_{Y_n}(x) = \Prob[Y_n \le x] = \Prob [X_1, X_2, \ldots, X_n \le x] = \Prob [X_1 \le x]^n = x^n.
	\end{align*}
	The PDF of $Y_n$ is therefore $f_{Y_n}(x)\frac{\dd}{\dd x} \left[ x^n \right] = n x^{n - 1}$, and so
	\begin{align*}
		\E[Y_n] = \int f_{Y_n}(x) \cdot x \dd x = \int_0^{1} n x^{n - 1} \cdot x \dd x= \int_0^1 nx^n \dd x = \frac{n}{n + 1}.
	\end{align*}
\end{Solution}
\begin{Exercise}
	A particle starts at the origin of the real line and moves along the line in jumps of one unit. For each jump the probability is $p$ that the particle will jump one unit to the left and the probability is $1 - p$ that the particle will jump one unit to the right. Let $X_n$ be theposition of the particle after $n$ units. Find $\E[X_n]$ and $\Var[X_n]$.
\end{Exercise}
\begin{Solution}
	When $n = 0$, $\E[X_n] = 0$, and when $n > 0$,
	\begin{align*}
		\E[X_n] &= \E[p \cdot (X_{n - 1} - 1)] + \E[(1 - p) \cdot (X_{n - 1} + 1)]
		     \\ &= p \E[X_{n - 1}] - p + (1 - p) \E[X_{n - 1}] + 1 - p
		     \\ &= \E[X_{n - 1}] + 1 - 2p.
	\end{align*}
	Therefore, by induction $\E[X_n] = (1 - 2p) \cdot n.$ 

	As for the variance, let $Y_i$ denote whether or not you move left ($-1$) or right ($+1$) on the $i$th jump.Then,
	\begin{align*}
		\Var[X_n] &= \Var[Y_1 + Y_2 + \ldots + Y_n]
		       \\ &= n \Var[Y_1]
		       \\ &= n (4p^2 - 4p).
	\end{align*}
\end{Solution}
\begin{Exercise}
	A fair coin is tossed until a head is obtained. What is the expected number of tosses that will be required?
\end{Exercise}
\begin{Solution}
	If we let $X_i$ denote the event that the first time we get heads is on the $i$th toss, then $\Prob[X_i] = \frac{1}{2^i}$, so
	\begin{align*}
		\E[X] = \sum_{i = 1}^{\infty} \frac{i}{2^i} = 2,
	\end{align*}
	where $X$ denotes the number of tosses required.
\end{Solution}
\begin{Exercise}
\end{Exercise}
\begin{Exercise}
\end{Exercise}
\begin{Exercise}
	Prove theorem 3.17.
\end{Exercise}
\begin{Solution}
	By linearity of expectation
	\begin{align*}
		\E[\overline{X}_n] = \E \left[ \frac{X_1 + X_2 + \ldots + X_n}{n} \right] = \frac{\E[X_1] + \E[X_2] + \ldots + \E[X_n]}{n} = \frac{n \mu}{n} = \mu.
	\end{align*}
	Additionally,
	\begin{align*}
		\Var[\overline{X}_n] = \Var \left[ \frac{X_1 + \ldots + X_n}{n} \right] = \frac{1}{n^2} \Var[X_1 + \ldots + X_n] = \frac{1}{n^2} \left( \Var[X_1] + \ldots + \Var[X_n] \right) = \frac{\sigma^2}{n^2}.
	\end{align*}
	The expected value of the sample variance is
	\begin{align*}
		S_n &= \E \left[ \frac{1}{n - 1} \sum_{i = 1}^n (X_i - \overline{X}_n)^2 \right]
		 \\ &= \frac{n}{n - 1} \cdot \mathbb{E}\left[ \left( X_1 - \overline{X}_n \right)^2 \right]
		 \\ &= \frac{1}{n(n - 1)}\cdot \E \left[ \left( (n - 1)X_1 - X_2 - X_3\ldots - X_n \right)^2 \right].
	\end{align*}
	If we let $Y = (n - 1)X_1 - X_2 - \ldots - X_n$, then 
	\begin{align*}
		\Var[Y] = \Var[(n - 1)X_1] + \Var[X_2] + \ldots + \Var[X_n] = (n - 1)^2 \sigma + (n - 1) \sigma = n (n - 1) \sigma,
	\end{align*}
	so $\E[Y^2] = \Var[Y] + \E[Y]^2 = n (n - 1) \sigma^2$. Plugging this into the prior equation, $\E[S_n^2] = \frac{1}{n (n - 1)} \cdot n(n -1)\sigma^2 = \sigma^2.$
\end{Solution}
\begin{Exercise}
\end{Exercise}
\begin{Exercise}
	Let $X \sim \Norm[0, 1]$ and let $Y = e^{X}$. Find $\E[Y]$ and $\Var[Y].$ 
\end{Exercise}
\begin{Solution}
	The MGF of $X$ is $\psi_X(t) = \E[e^{Xt}] = e^{\frac{1}{2}t^2}$, so 
	$\E[Y] = \sqrt{e}$ and $\E[Y^2] = e^2$. It follows that $\Var[Y] = \E[Y^2] - \E[Y]^2 = e^2 - e.$
\end{Solution}
\end{document}
