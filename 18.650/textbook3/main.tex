% \documentclass{article}
\documentclass[a4paper]{article}
\usepackage{/Users/mariachrysafis/Documents/Schoolwork/18.650/chrysafis}
\graphicspath{ {/} }
\author{Maria Chrysafis}
\date{\today}
\title{All of Statistics Chapter 3}
\begin{document}
\maketitle
\section{Expectation}
\begin{Exercise}
	Suppose we play a game where we start with $c$ dollars. On each play of the game, you either double or halve your money, with equal probability. What is your expected fortune after $n$ trials?
\end{Exercise}
\begin{Solution}
	Let $X_i$ denote the amount of money you have after playing the game $i$ times. When $i = 0$, by definition, $\Prob[X_0 = c] = 1$, and so, $\E[X_0] = c$. When $i > 0$, 
	\begin{align*}
		\E[X_i] = \E \left[ \frac{1}{2} \cdot \left(2 \cdot X_{i - 1} \right) + \frac{1}{2} \cdot \left( \frac{1}{2} \cdot X_i \right) \right] = \E \left[\frac{5}{4} X_{i - 1} \right] = \frac{5}{4} \E \left[ X_{i - 1} \right].
	\end{align*}
	It immediately follows that $\E[X_i] = \left( \frac{5}{4} \right)^i \cdot c$. Thus, after $n$ trials, your expected fortune is $c \cdot \left( \frac{5}{4}\right)^n$.
\end{Solution}
\begin{Exercise}
	Show that $\Var[X] = 0$ if and only if there is a constant $c$ such that $\Prob[X = c]= 1.$
\end{Exercise}
\begin{Solution}
	We first prove the easier direction, namely that if $\Prob[X = c] = 1$, then $\Var[X] = 0$. In this case, $\E[X^2] = c^2$ and $\E[X]^2 = c^2$ too, so $\Var[X] = \E[X^2] - \E[X]^2 = 0$, as desired. 

	As for the other direction, by Jensen's inequality,
	\begin{align*}
		\mathbb{E}[X^2] \ge \mathbb{E}[X]^2 \Longleftrightarrow \Var[X] \ge 0,
	\end{align*}
	with equality holding iff $X$ is constant, i.e. $\Prob[X = c] = 1$ for some $c$. 
\end{Solution}
\begin{Exercise}
	Let $X_1, \ldots , X_n \sim \Unif[0, 1]$ and let $Y_n = \max \left( X_1, \ldots X_n \right)$. Find $\mathbb{E}[Y_n]$.
\end{Exercise}
\begin{Solution}
	Consider the CDF of $Y_n$:
	\begin{align*}
		F_{Y_n}(x) = \Prob[Y_n \le x] = \Prob [X_1, X_2, \ldots, X_n \le x] = \Prob [X_1 \le x]^n = x^n.
	\end{align*}
	The PDF of $Y_n$ is therefore $f_{Y_n}(x)\frac{\dd}{\dd x} \left[ x^n \right] = n x^{n - 1}$, and so
	\begin{align*}
		\E[Y_n] = \int f_{Y_n}(x) \cdot x \dd x = \int_0^{1} n x^{n - 1} \cdot x \dd x= \int_0^1 nx^n \dd x = \frac{n}{n + 1}.
	\end{align*}
\end{Solution}
\begin{Exercise}
	A particle starts at the origin of the real line and moves along the line in jumps of one unit. For each jump the probability is $p$ that the particle will jump one unit to the left and the probability is $1 - p$ that the particle will jump one unit to the right. Let $X_n$ be theposition of the particle after $n$ units. Find $\E[X_n]$ and $\Var[X_n]$.
\end{Exercise}
\begin{Solution}
	When $n = 0$, $\E[X_n] = 0$, and when $n > 0$,
	\begin{align*}
		\E[X_n] &= \E[p \cdot (X_{n - 1} - 1)] + \E[(1 - p) \cdot (X_{n - 1} + 1)]
		     \\ &= p \E[X_{n - 1}] - p + (1 - p) \E[X_{n - 1}] + 1 - p
		     \\ &= \E[X_{n - 1}] + 1 - 2p.
	\end{align*}
	Therefore, by induction $\E[X_n] = (1 - 2p) \cdot n.$ 

	As for the variance, let $Y_i$ denote whether or not you move left ($-1$) or right ($+1$) on the $i$th jump.Then,
	\begin{align*}
		\Var[X_n] &= \Var[Y_1 + Y_2 + \ldots + Y_n]
		       \\ &= n \Var[Y_1]
		       \\ &= n (4p^2 - 4p).
	\end{align*}
\end{Solution}
\begin{Exercise}
	A fair coin is tossed until a head is obtained. What is the expected number of tosses that will be required?
\end{Exercise}
\begin{Solution}
	If we let $X_i$ denote the event that the first time we get heads is on the $i$th toss, then $\Prob[X_i] = \frac{1}{2^i}$, so
	\begin{align*}
		\E[X] = \sum_{i = 1}^{\infty} \frac{i}{2^i} = 2,
	\end{align*}
	where $X$ denotes the number of tosses required.
\end{Solution}
\begin{Exercise}
	Prove theorem 3.6 for discrete random variables.
\end{Exercise}
\begin{Solution}
	For discrete random variables, the expected value of $X$ is
	\begin{align*}
		\sum_i \Prob[X = x_i] x_i,
	\end{align*}
	so the expected value of $r(X)$ is 
	\begin{align*}
		\sum_i \Prob[r(X) = r_i] r_i = \sum_i \Prob[r(X) = r(x_i)] x_i.
	\end{align*}
\end{Solution}
\begin{Exercise}
	Let $X$ be a continous random variable with CDF $F$. Suppose that $\Prob[X >0] = 1$ and that $\E[X]$ exists. Show that $\E[X] = \int_0^{\infty} \Prob[X > x] \dd x.$
\end{Exercise}
\begin{Solution}
We know that $\Prob[X > x]= \int_{x}^{\infty} f_X(y) \dd y$, so
\begin{align*}
	\int_0^{\infty} \Prob[X > x] \dd x &= \int_0^{\infty} \int_x^{\infty} f_X(y) \dd y \dd x 
					\\ &= \int_0^{\infty} \int_{0}^{y} f_X(y) \dd x \dd y
					\\ &= \int_0^{\infty} f_X(y) y \dd y
					\\ &= \mathbb{E}[X],
\end{align*}
from which the desired follows.
\end{Solution}
\begin{Exercise}
	Prove theorem 3.17.
\end{Exercise}
\begin{Solution}
	By linearity of expectation
	\begin{align*}
		\E[\overline{X}_n] = \E \left[ \frac{X_1 + X_2 + \ldots + X_n}{n} \right] = \frac{\E[X_1] + \E[X_2] + \ldots + \E[X_n]}{n} = \frac{n \mu}{n} = \mu.
	\end{align*}
	Additionally,
	\begin{align*}
		\Var[\overline{X}_n] = \Var \left[ \frac{X_1 + \ldots + X_n}{n} \right] = \frac{1}{n^2} \Var[X_1 + \ldots + X_n] = \frac{1}{n^2} \left( \Var[X_1] + \ldots + \Var[X_n] \right) = \frac{\sigma^2}{n^2}.
	\end{align*}
	The expected value of the sample variance is
	\begin{align*}
		S_n &= \E \left[ \frac{1}{n - 1} \sum_{i = 1}^n (X_i - \overline{X}_n)^2 \right]
		 \\ &= \frac{n}{n - 1} \cdot \mathbb{E}\left[ \left( X_1 - \overline{X}_n \right)^2 \right]
		 \\ &= \frac{1}{n(n - 1)}\cdot \E \left[ \left( (n - 1)X_1 - X_2 - X_3\ldots - X_n \right)^2 \right].
	\end{align*}
	If we let $Y = (n - 1)X_1 - X_2 - \ldots - X_n$, then 
	\begin{align*}
		\Var[Y] = \Var[(n - 1)X_1] + \Var[X_2] + \ldots + \Var[X_n] = (n - 1)^2 \sigma + (n - 1) \sigma = n (n - 1) \sigma,
	\end{align*}
	so $\E[Y^2] = \Var[Y] + \E[Y]^2 = n (n - 1) \sigma^2$. Plugging this into the prior equation, $\E[S_n^2] = \frac{1}{n (n - 1)} \cdot n(n -1)\sigma^2 = \sigma^2.$
\end{Solution}
\begin{Exercise}
	Let $X_1, X_2, \ldots X_n$ be $\Norm[0, 1]$ random variables and let $\overline{X}_n = n^{-1} \sum_{i = 1}^{n} X_i$. Plot $\overline{X}_n$ verus $n$ for $n = 1, \ldots , 10,000$. Repeat for $X_1, X_2, \ldots X_n \sim \Cauchy$. Explain why there is such a difference.
\end{Exercise}
\begin{Solution}
	See the $\texttt{.ipynb}$ file. Expectedly, the sample mean for the normal distribution appears to converge to $1$, but the sample mean of the Cauchy sequence does not converge (since the Cauchy sequence is so fat-tailed, it does not have a well-defined mean).
\end{Solution}
\begin{Exercise}
	Let $X \sim \Norm[0, 1]$ and let $Y = e^{X}$. Find $\E[Y]$ and $\Var[Y].$ 
\end{Exercise}
\begin{Solution}
	The MGF of $X$ is $\psi_X(t) = \E[e^{Xt}] = e^{\frac{1}{2}t^2}$, so 
	$\E[Y] = \sqrt{e}$ and $\E[Y^2] = e^2$. It follows that $\Var[Y] = \E[Y^2] - \E[Y]^2 = e^2 - e.$
\end{Solution}
\begin{Exercise}
	Let $Y_1, Y_2, \ldots $ be independent random variables such that $\Prob[Y_i = 1] = \Prob[Y_i = -1] = \frac{1}{2}$. Let $X_n = \sum_{i = 1}^n Y_i.$ Think of $Y_i$ as \enquote{the stock price increased by one dollar}, $Y_i = -1$ as \enquote{the stock price decreased by one dollar}, and $X_n$ as the value of the stock on day $n$.
	\begin{itemize}
		\item[(a)] Find $\E[X_n]$ and $\Var[X_n]$.
		\item[(b)] Stumlate $X_n$ and plot $X_n$ versus $n$ for $n = 1, 2, \ldots 10,000$. Repeat the whole stimulation several times. Notice two things. First, it is easy to see patterns in the sequence even though it is random. Second, you will find that the four runs look very different even though they were generated the same way. How do the calcualtions in (a) explain the second observation?
	\end{itemize}
\end{Exercise}
\begin{Solution}
	\begin{itemize}
		\item[(a)] 
		\begin{align*}
			\E[X_n] &= \E \left[ \sum_{i = 1}^n Y_i \right] = \sum_{i = 1}^n \E[Y_i] = 0
		\end{align*}
		\begin{align*}
			\Var[X_n] &= \Var \left[ \sum_{i = 1}^n Y_i \right] = \sum_{i = 1}^n \Var[Y_i] = \sum_{i = 1}^n 1 = n.
		\end{align*}
		\item[(b)] See the $\texttt{.ipynb}$ file. There is expectedly, high variance, since the variance is a linear factor of $n$.
	\end{itemize}
\end{Solution}
\end{document}